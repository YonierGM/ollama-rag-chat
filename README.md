# ğŸ” RAG Local API - FastAPI + LangChain + Ollama + Chroma

Este proyecto implementa una API local de RecuperaciÃ³n Aumentada por GeneraciÃ³n (RAG) usando:

- ğŸ§  **FastAPI** para el backend
- ğŸ”— **LangChain** para orquestar la lÃ³gica RAG
- ğŸ¦™ **Ollama** para correr modelos LLM y de embeddings localmente
- ğŸ§  **ChromaDB** como base vectorial para recuperaciÃ³n semÃ¡ntica
- ğŸ³ **Docker Compose** para contenerizar todo
- ğŸŒ **Redis** para almacenamiento temporal del historial de conversaciÃ³n.

## ğŸ³ Requisitos previos
- [Docker](https://www.docker.com/)
---

## ğŸš€ Funcionalidades principales

âœ… Ingesta de documentos (.pdf, .docx, .txt)
âœ… FragmentaciÃ³n automÃ¡tica de texto (con RecursiveCharacterTextSplitter)
âœ… IndexaciÃ³n en base vectorial con embeddings (mxbai-embed-large)
âœ… RecuperaciÃ³n contextual con bÃºsqueda MMR
âœ… GeneraciÃ³n de respuestas enriquecidas con contexto y memoria
âœ… Soporte multi-modelo: puedes elegir entre distintos modelos Ollama
âœ… Endpoints para listar, resetear y gestionar historial
âœ… DocumentaciÃ³n automÃ¡tica con Swagger UI

## Endpoints principales
- `POST /ingest`: Subida y fragmentaciÃ³n de documentos
- `POST /ask_model`: Consulta a un modelo con contexto y memoria
- `GET /models`: Lista los modelos de chat disponibles en Ollama
- `GET /history`: Obtiene el historial de conversaciÃ³n
- `POST /clearHistory`: Elimina el historial
- `DELETE /reset_embeddings`: Elimina la base vectorial completa

## âš™ï¸ TecnologÃ­as principales

- LLM: `llama3.2:latest` o el de tu preferencia
- Embeddings: `mxbai-embed-large`
- Backend API: FastAPI
- Vector store: Chroma
- GestiÃ³n de prompts y memoria contextual: LangChain

---


## ğŸš€ Paso a paso para levantar el proyecto

### 1. Clona el repositorio

```bash
git clone https://github.com/YonierGM/ollama-rag-chat
cd ollama-rag-chat
```

### 2. Construir contenedores y levantar proyecto
```bash
docker compose up --build
```

### 3. Instalar modelos en el contenedor de ollama
- Modelo de embeddings `mxbai-embed-large`
- modelo LLM `llama3.2`

Accede al contenedor de ollama `Containers/ollama-ai` y ejecutar los comandos:

### Referencia
![Contenedor-ollama](images/Contenedor.png)

Comandos:
```bash
ollama pull mxbai-embed-large
ollama run llama3.2
```

Una vez que hayas instalado los modelos de embeddings y LLM, puedes elegir cuÃ¡l utilizar desde el selector ubicado en la esquina inferior izquierda de la interfaz. SiÃ©ntete libre de probar diferentes modelos que hayas instalado.

### Interfaz principal
![Inicio-App](images/inicio-app.png)

### embeddings y borrado
![Configuraciones](images/Configuraciones.png)

